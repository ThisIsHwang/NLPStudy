# 인공신경망 훑어보기

## 1. 피드 포워드 신경망 (FFNN)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5f5933cb-f7a0-4d24-bab7-7ad87dae3fab/Untitled.png)

## 2. ****전결합층****

## 3. ****활성화 함수(Activation Function)****

뉴런에서 출력값을 결정하는 함수를 활성화 함수(Activation function)라고 한다.

💡함수 그리는 Package Import

```python
import numpy as np
import matplotlib.pyplot as plt
```

### 1.****계단 함수(Step function)****

```python
def step(x):
    return np.array(x > 0, dtype=np.int)
x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = step(x)
plt.title('Step Function')
plt.plot(x,y)
plt.show()
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bc9bd65f-c987-497e-b70f-064718e2034b/Untitled.png)

### 2.****시그모이드 함수(Sigmoid function)****

```python
def sigmoid(x):
    return 1/(1+np.exp(-x))
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)

plt.plot(x, y)
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fb43ca4a-4d57-43aa-b34e-99f1adf10b9d/Untitled.png)

### 3.****하이퍼볼릭탄젠트 함수(Hyperbolic tangent function)****

```python
x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = np.tanh(x)

plt.plot(x, y)
plt.plot([0,0],[1.0,-1.0], ':')
plt.axhline(y=0, color='orange', linestyle='--')
plt.title('Tanh Function')
plt.show()
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/04bc6a12-548b-4423-8e03-cb344792a518/Untitled.png)

### 4.****렐루 함수(ReLU)****

```python
def relu(x):
    return np.maximum(0, x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)

plt.plot(x, y)
plt.plot([0,0],[5.0,0.0], ':')
plt.title('Relu Function')
plt.show()
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/99a2beef-c1c7-475d-b2e4-7105c4b4ff31/Untitled.png)

### 5.****리키 렐루(Leaky ReLU)****

```python
a = 0.1

def leaky_relu(x):
    return np.maximum(a*x, x)

x = np.arange(-5.0, 5.0, 0.1)
y = leaky_relu(x)

plt.plot(x, y)
plt.plot([0,0],[5.0,0.0], ':')
plt.title('Leaky ReLU Function')
plt.show()
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/72726d41-23d8-4e60-9cbd-24f157176f57/Untitled.png)

### 6.****소프트맥스 함수(Softamx function)****

```python
x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = np.exp(x) / np.sum(np.exp(x))

plt.plot(x, y)
plt.title('Softmax Function')
plt.show()
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f4378346-8f90-4570-8d0e-f2ae5d6ce939/Untitled.png)